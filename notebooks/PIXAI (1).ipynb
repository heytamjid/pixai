{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13948105,
          "sourceType": "datasetVersion",
          "datasetId": 8890113
        },
        {
          "sourceId": 13952607,
          "sourceType": "datasetVersion",
          "datasetId": 8893279
        },
        {
          "sourceId": 13952938,
          "sourceType": "datasetVersion",
          "datasetId": 8893521
        },
        {
          "sourceId": 13952971,
          "sourceType": "datasetVersion",
          "datasetId": 8893550
        },
        {
          "sourceId": 13977804,
          "sourceType": "datasetVersion",
          "datasetId": 8910675
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbTJdBPzalS-",
        "outputId": "d7788e7e-03c3-4d67-e255-2a75403c8119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        pass\n",
        "        #print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:38:24.188430Z",
          "iopub.execute_input": "2025-12-03T19:38:24.188740Z",
          "iopub.status.idle": "2025-12-03T19:38:32.933209Z",
          "shell.execute_reply.started": "2025-12-03T19:38:24.188707Z",
          "shell.execute_reply": "2025-12-03T19:38:32.932589Z"
        },
        "id": "-2yl4wXJaXtY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "N4YhTylXaXtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:38:32.934330Z",
          "iopub.execute_input": "2025-12-03T19:38:32.934688Z",
          "iopub.status.idle": "2025-12-03T19:39:58.378202Z",
          "shell.execute_reply.started": "2025-12-03T19:38:32.934670Z",
          "shell.execute_reply": "2025-12-03T19:39:58.376970Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecwA0pfuaXtZ",
        "outputId": "c4e8ea0f-a380-4a10-a3a2-ed2d3e250555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-u5asvjn1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-u5asvjn1\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split  # Added for splitting\n",
        "from transformers import AutoTokenizer\n",
        "import clip\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:39:58.379420Z",
          "iopub.execute_input": "2025-12-03T19:39:58.380178Z",
          "iopub.status.idle": "2025-12-03T19:40:10.196857Z",
          "shell.execute_reply.started": "2025-12-03T19:39:58.380147Z",
          "shell.execute_reply": "2025-12-03T19:40:10.196055Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG4hUKfJaXta",
        "outputId": "25813781-aea1-42bd-f527-eec082febb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install madgrad"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:10.198347Z",
          "iopub.execute_input": "2025-12-03T19:40:10.198773Z",
          "iopub.status.idle": "2025-12-03T19:40:19.956126Z",
          "shell.execute_reply.started": "2025-12-03T19:40:10.198751Z",
          "shell.execute_reply": "2025-12-03T19:40:19.955187Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMnNGeVqaXta",
        "outputId": "31240645-75ad-4969-e8e7-af5c13674e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: madgrad in /usr/local/lib/python3.12/dist-packages (1.3)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from madgrad import MADGRAD"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:19.959323Z",
          "iopub.execute_input": "2025-12-03T19:40:19.959994Z",
          "iopub.status.idle": "2025-12-03T19:40:20.028947Z",
          "shell.execute_reply.started": "2025-12-03T19:40:19.959955Z",
          "shell.execute_reply": "2025-12-03T19:40:20.028308Z"
        },
        "id": "V8Cd6mRaaXta"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Notebook"
      ],
      "metadata": {
        "id": "VkKhXHafaXta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_weigths(data):\n",
        "    # Ensure labels are integers\n",
        "    y = data['Labels'].astype(int)\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "    return class_weights"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:20.029741Z",
          "iopub.execute_input": "2025-12-03T19:40:20.029981Z",
          "iopub.status.idle": "2025-12-03T19:40:20.035587Z",
          "shell.execute_reply.started": "2025-12-03T19:40:20.029960Z",
          "shell.execute_reply": "2025-12-03T19:40:20.034810Z"
        },
        "id": "E7_Lru6LaXta"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class BHMDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, data_dir, max_seq_length, transform=None):\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.data_dir = data_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. Load Image\n",
        "        img_filename = self.data.loc[idx, 'image_name']\n",
        "        img_path = os.path.join(self.data_dir, img_filename)\n",
        "\n",
        "        # Open and convert to RGB (handles PNGs with transparency issues)\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except (OSError, FileNotFoundError):\n",
        "            # Fallback for missing images - create black image\n",
        "            print(f\"Warning: Could not open {img_path}. Using blank image.\")\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        # 2. Load Text\n",
        "        caption = str(self.data.loc[idx, 'Captions'])\n",
        "\n",
        "        # 3. Load Label\n",
        "        label = int(self.data.loc[idx, 'Labels'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(caption, return_tensors='pt',\n",
        "                                padding='max_length', truncation=True, max_length=self.max_seq_length)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'label': label\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:20.036314Z",
          "iopub.execute_input": "2025-12-03T19:40:20.036605Z",
          "iopub.status.idle": "2025-12-03T19:40:20.055161Z",
          "shell.execute_reply.started": "2025-12-03T19:40:20.036581Z",
          "shell.execute_reply": "2025-12-03T19:40:20.054468Z"
        },
        "id": "ybOpyuM6aXtb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Datasets.py\n",
        "\n",
        "# def load_dataset_merged(csv_file_path, memes_path, max_len, batch_size):\n",
        "#     print(f\"Loading Single Merged Dataset from {csv_file_path}...\")\n",
        "#     print(\"Splitting: 80% Train / 20% Val\")\n",
        "\n",
        "#     # 1. Read the Single CSV\n",
        "#     df = pd.read_csv(csv_file_path)\n",
        "\n",
        "#     # 2. Rename columns to match BHMDataset expectations\n",
        "#     # Your CSV has: 'Image_name', 'extracted_text', 'Label'\n",
        "#     # BHMDataset needs: 'image_name', 'Captions', 'Labels'\n",
        "\n",
        "#     df = df.rename(columns={\n",
        "#         'Image_name': 'image_name',      # Fixes capitalization\n",
        "#         'normalized_corpus': 'Captions',    # Maps text column\n",
        "#         'Label': 'Labels'                # Maps label column\n",
        "#     })\n",
        "\n",
        "#     # 3. Handle Missing Text\n",
        "#     df['Captions'] = df['Captions'].fillna(\"\")\n",
        "\n",
        "#     # 4. Map Labels (Binary)\n",
        "#     # Ensure your CSV uses \"NonPolitical\" and \"Political\" exactly\n",
        "#     label_map = {\"NonPolitical\": 0, \"Political\": 1}\n",
        "#     df['Labels'] = df['Labels'].map(label_map)\n",
        "\n",
        "#     # Drop rows where label mapping failed (e.g., if there's a typo in the CSV)\n",
        "#     df = df.dropna(subset=['Labels'])\n",
        "\n",
        "#     # 5. Split Data (80% Train, 20% Validation)\n",
        "#     # We stratify by 'Labels' to keep the ratio of Political/NonPolitical consistent\n",
        "#     train_df, val_df = train_test_split(\n",
        "#         df,\n",
        "#         test_size=0.20,\n",
        "#         random_state=42,\n",
        "#         stratify=df['Labels']\n",
        "#     )\n",
        "\n",
        "#     print(f\"Total Samples: {len(df)}\")\n",
        "#     print(f\"Training Samples: {len(train_df)}\")\n",
        "#     print(f\"Validation Samples: {len(val_df)}\")\n",
        "\n",
        "#     # 6. Weights & Tokenizer\n",
        "#     # Calculate weights to handle class imbalance\n",
        "#     weights = balanced_weigths(train_df)\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n",
        "\n",
        "#     data_transform = transforms.Compose([\n",
        "#         transforms.Resize((224, 224)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "#     ])\n",
        "\n",
        "#     # 7. Create Loaders\n",
        "#     train_dataset = BHMDataset(dataframe=train_df, tokenizer=tokenizer, data_dir=memes_path,\n",
        "#                                max_seq_length=max_len, transform=data_transform)\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#     val_dataset = BHMDataset(dataframe=val_df, tokenizer=tokenizer, data_dir=memes_path,\n",
        "#                              max_seq_length=max_len, transform=data_transform)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#     # Return only train, val, and weights\n",
        "#     return train_loader, val_loader, weights"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:20.055774Z",
          "iopub.execute_input": "2025-12-03T19:40:20.055957Z",
          "iopub.status.idle": "2025-12-03T19:40:20.071643Z",
          "shell.execute_reply.started": "2025-12-03T19:40:20.055943Z",
          "shell.execute_reply": "2025-12-03T19:40:20.070972Z"
        },
        "id": "e8olamk3aXtb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets.py\n",
        "\n",
        "def load_dataset_merged(csv_file_path, memes_path, max_len, batch_size):\n",
        "    print(f\"Loading Single Merged Dataset from {csv_file_path}...\")\n",
        "    print(\"Splitting: 70% Train / 15% Val / 15% Test\")\n",
        "\n",
        "    # 1. Read the Single CSV\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # 2. Rename columns to match BHMDataset expectations\n",
        "    df = df.rename(columns={\n",
        "        'Image_name': 'image_name',\n",
        "        'normalized_corpus': 'Captions',\n",
        "        'Label': 'Labels'\n",
        "    })\n",
        "\n",
        "    # 3. Handle Missing Text\n",
        "    df['Captions'] = df['Captions'].fillna(\"\")\n",
        "\n",
        "    # 4. Map Labels (Binary)\n",
        "    label_map = {\"NonPolitical\": 0, \"Political\": 1}\n",
        "    df['Labels'] = df['Labels'].map(label_map)\n",
        "    df = df.dropna(subset=['Labels'])\n",
        "\n",
        "    # 5. Split Data (70-15-15)\n",
        "    # Step 1: Split into Train (70%) and Temp (30%)\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df,\n",
        "        test_size=0.30,\n",
        "        random_state=42,\n",
        "        stratify=df['Labels']\n",
        "    )\n",
        "\n",
        "    # Step 2: Split Temp (30%) into Val (15%) and Test (15%)\n",
        "    # Since 0.15 is half of 0.30, we use test_size=0.5 on the temp_df\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=0.50,\n",
        "        random_state=42,\n",
        "        stratify=temp_df['Labels']\n",
        "    )\n",
        "\n",
        "    print(f\"Total Samples: {len(df)}\")\n",
        "    print(f\"Training Samples: {len(train_df)}\")\n",
        "    print(f\"Validation Samples: {len(val_df)}\")\n",
        "    print(f\"Test Samples: {len(test_df)}\")\n",
        "\n",
        "    # 6. Weights & Tokenizer\n",
        "    weights = balanced_weigths(train_df)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n",
        "\n",
        "    # Transforms (Val and Test use the same standard transform)\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # 7. Create Loaders\n",
        "    train_dataset = BHMDataset(dataframe=train_df, tokenizer=tokenizer, data_dir=memes_path,\n",
        "                               max_seq_length=max_len, transform=data_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_dataset = BHMDataset(dataframe=val_df, tokenizer=tokenizer, data_dir=memes_path,\n",
        "                             max_seq_length=max_len, transform=data_transform)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    test_dataset = BHMDataset(dataframe=test_df, tokenizer=tokenizer, data_dir=memes_path,\n",
        "                             max_seq_length=max_len, transform=data_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Return Train, Val, Test, and Weights\n",
        "    return train_loader, val_loader, test_loader, weights"
      ],
      "metadata": {
        "id": "brQ70zPqumLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ],
      "metadata": {
        "id": "r5QALH0eaXtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:20.072281Z",
          "iopub.execute_input": "2025-12-03T19:40:20.072673Z",
          "iopub.status.idle": "2025-12-03T19:40:20.095183Z",
          "shell.execute_reply.started": "2025-12-03T19:40:20.072646Z",
          "shell.execute_reply": "2025-12-03T19:40:20.094545Z"
        },
        "id": "9sMr_uEfaXtb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. UTILS & ATTENTION ---\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        output, _ = self.attention(query, key, value, attn_mask=mask)\n",
        "        return output\n",
        "\n",
        "# --- 2. MODEL DEFINITION ---\n",
        "\n",
        "# Load CLIP once\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "clip_model = clip_model.visual.float().to(device)\n",
        "\n",
        "# Freeze CLIP\n",
        "for param in clip_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "class PIXAI(nn.Module):\n",
        "    def __init__(self, clip_model, num_classes, num_heads, max_len):\n",
        "        super(PIXAI, self).__init__()\n",
        "\n",
        "        self.max_len = max_len # dynamic sequence length\n",
        "\n",
        "        # Visual feature extractor (CLIP)\n",
        "        self.clip = clip_model\n",
        "        self.visual_linear = nn.Linear(512, 1024)\n",
        "\n",
        "        # Textual feature extractor (BERT/XGLM)\n",
        "        self.bert = AutoModel.from_pretrained(\"facebook/xglm-564M\")\n",
        "\n",
        "        # Multihead attention\n",
        "        self.attention = MultiheadAttention(d_model=1024, nhead=num_heads)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(1024 + 1024 + 1024 + 1024, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.01),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image_input, input_ids, attention_mask):\n",
        "        # Extract visual features\n",
        "        image_features = self.clip(image_input)\n",
        "        image_features = self.visual_linear(image_features)\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "\n",
        "        # Dynamic pooling based on max_len\n",
        "        image_features = F.adaptive_avg_pool1d(image_features.permute(0, 2, 1), self.max_len).permute(0, 2, 1)\n",
        "\n",
        "        # Extract Text features\n",
        "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        bert_output = bert_outputs.last_hidden_state\n",
        "\n",
        "        # Attention 1: Image querying Text\n",
        "        attention_output1 = self.attention(\n",
        "            query=image_features.permute(1, 0, 2),\n",
        "            key=bert_output.permute(1, 0, 2),\n",
        "            value=bert_output.permute(1, 0, 2)\n",
        "        ).permute(1, 0, 2)\n",
        "\n",
        "        # Attention 2: Image querying Image (Self/Cross hybrid)\n",
        "        attention_output2 = self.attention(\n",
        "            query=image_features.permute(1, 0, 2),\n",
        "            key=bert_output.permute(1, 0, 2),\n",
        "            value=image_features.permute(1, 0, 2)\n",
        "        ).permute(1, 0, 2)\n",
        "\n",
        "        # Fusion\n",
        "        fusion_input = torch.cat([attention_output1, attention_output2, image_features, bert_output], dim=2)\n",
        "\n",
        "        output = self.fc(fusion_input.mean(1))\n",
        "        return output"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:20.095910Z",
          "iopub.execute_input": "2025-12-03T19:40:20.096136Z",
          "iopub.status.idle": "2025-12-03T19:40:27.963783Z",
          "shell.execute_reply.started": "2025-12-03T19:40:20.096120Z",
          "shell.execute_reply": "2025-12-03T19:40:27.962909Z"
        },
        "id": "rjhNdG20aXtb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "DLwm7updaXtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predictions, targets):\n",
        "    # Apply Sigmoid because model outputs logits\n",
        "    probs = torch.sigmoid(predictions)\n",
        "    preds = (probs > 0.5).float()\n",
        "    correct = (preds == targets).float()\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:27.964635Z",
          "iopub.execute_input": "2025-12-03T19:40:27.964911Z",
          "iopub.status.idle": "2025-12-03T19:40:27.970494Z",
          "shell.execute_reply.started": "2025-12-03T19:40:27.964886Z",
          "shell.execute_reply": "2025-12-03T19:40:27.969822Z"
        },
        "id": "0btCVHMKaXtc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ],
      "metadata": {
        "id": "LOQ8MV5DaXtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, val_loader, task, path, heads, class_weights, epochs, lr_rate):\n",
        "\n",
        "    # 1. Get max_len from dataset\n",
        "    current_max_len = train_loader.dataset.max_seq_length\n",
        "    print(f\"Detected Max Sequence Length: {current_max_len}\")\n",
        "\n",
        "    # 2. Binary Classification Setup\n",
        "    num_classes = 1\n",
        "\n",
        "    # Handle Class Imbalance with Pos_Weight\n",
        "    if class_weights is not None:\n",
        "        # Assuming class_weights is [weight_0, weight_1]\n",
        "        pos_weight = class_weights[1]\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        print(f\"Using Weighted Loss (Pos Weight: {pos_weight:.4f})\")\n",
        "    else:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # 3. Initialize Model\n",
        "    # Ensure clip_model and device are defined globally or passed in\n",
        "    model = PIXAI(clip_model, num_classes=num_classes, num_heads=heads, max_len=current_max_len)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 4. Optimization\n",
        "    optimizer = MADGRAD(model.parameters(), lr=lr_rate)\n",
        "\n",
        "    num_training_steps = epochs * len(train_loader)\n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Track best performance (using Macro F1 is usually best for balance)\n",
        "    best_val_metric = 0.0\n",
        "\n",
        "    print(f\"Start Training PIXAI (Binary Mode)\")\n",
        "    print(\"--------------------------------\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --- TRAINING PHASE ---\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Reset metric lists for this epoch\n",
        "        train_labels = []\n",
        "        train_preds = []\n",
        "\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", unit=\"batch\") as t:\n",
        "            for batch in t:\n",
        "                # Move data to device\n",
        "                images = batch['image'].to(device)\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].float().unsqueeze(1).to(device)\n",
        "\n",
        "                # Forward Pass\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images, input_ids, attention_mask)\n",
        "\n",
        "                # Loss Calculation\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Store Predictions for Metrics\n",
        "                # Apply Sigmoid to convert logits to probabilities [0,1]\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                preds = (probs > 0.5).float()\n",
        "\n",
        "                train_labels.extend(labels.cpu().detach().numpy().flatten())\n",
        "                train_preds.extend(preds.cpu().detach().numpy().flatten())\n",
        "\n",
        "                # Update Progress Bar\n",
        "                t.set_postfix(loss=total_loss / (t.n + 1))\n",
        "\n",
        "        # Calculate Training Metrics for the Epoch\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "        train_macro_f1 = f1_score(train_labels, train_preds, average='macro')\n",
        "\n",
        "        # --- VALIDATION PHASE ---\n",
        "        model.eval()\n",
        "        val_labels = []\n",
        "        val_preds = []\n",
        "        total_val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Valid]\", unit=\"batch\") as t:\n",
        "                for batch in t:\n",
        "                    images = batch['image'].to(device)\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    labels = batch['label'].float().unsqueeze(1).to(device)\n",
        "\n",
        "                    outputs = model(images, input_ids, attention_mask)\n",
        "\n",
        "                    val_loss = criterion(outputs, labels)\n",
        "                    total_val_loss += val_loss.item()\n",
        "\n",
        "                    # Convert Logits -> Probs -> Preds\n",
        "                    probs = torch.sigmoid(outputs)\n",
        "                    preds = (probs > 0.5).float()\n",
        "\n",
        "                    val_labels.extend(labels.cpu().numpy().flatten())\n",
        "                    val_preds.extend(preds.cpu().numpy().flatten())\n",
        "\n",
        "                    t.set_postfix(loss=total_val_loss / (t.n + 1))\n",
        "\n",
        "        # Calculate Validation Metrics\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "        # Detailed Metrics\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='binary')       # Target Class Performance\n",
        "        val_macro_f1 = f1_score(val_labels, val_preds, average='macro')  # Overall Balance\n",
        "\n",
        "        # --- PRINT REPORT ---\n",
        "        print(f\"\\n--- Results Epoch {epoch + 1} ---\")\n",
        "        print(f\"Train | Loss: {avg_train_loss:.4f} | Acc: {train_acc*100:.2f}% | Macro F1: {train_macro_f1:.4f}\")\n",
        "        print(f\"Valid | Loss: {avg_val_loss:.4f} | Acc: {val_acc*100:.2f}% | Macro F1: {val_macro_f1:.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # --- SAVE CHECKPOINT ---\n",
        "        # We save based on Macro F1 to ensure we aren't just predicting the majority class\n",
        "        if val_macro_f1 > best_val_metric:\n",
        "            best_val_metric = val_macro_f1\n",
        "\n",
        "            save_dict = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'max_len': current_max_len\n",
        "            }\n",
        "            torch.save(save_dict, os.path.join(path, f'model_{task}.pth'))\n",
        "            print(\">> Model Saved (Best Macro F1)\")\n",
        "\n",
        "        # Update Scheduler\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(f\"Best Validation Macro F1: {best_val_metric:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:27.971342Z",
          "iopub.execute_input": "2025-12-03T19:40:27.971587Z",
          "iopub.status.idle": "2025-12-03T19:40:28.003573Z",
          "shell.execute_reply.started": "2025-12-03T19:40:27.971562Z",
          "shell.execute_reply": "2025-12-03T19:40:28.002812Z"
        },
        "id": "2aJ2-ROcaXtc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "OLwtEG8QaXtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:28.004340Z",
          "iopub.execute_input": "2025-12-03T19:40:28.004590Z",
          "iopub.status.idle": "2025-12-03T19:40:32.127243Z",
          "shell.execute_reply.started": "2025-12-03T19:40:28.004569Z",
          "shell.execute_reply": "2025-12-03T19:40:32.126361Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZjBsMrPaXtc",
        "outputId": "a7e1b428-43e2-4956-a74e-c1ae88f88e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.12/dist-packages (3.20.3)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def get_detailed_metrics(model, test_loader, device):\n",
        "    print(\"\\n==========================================\")\n",
        "    print(\"Running Comprehensive Test Evaluation\")\n",
        "    print(\"==========================================\")\n",
        "\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # 1. Inference Loop\n",
        "    with torch.no_grad():\n",
        "        with tqdm(test_loader, desc=\"Testing\", unit=\"batch\") as t:\n",
        "            for batch in t:\n",
        "                images = batch['image'].to(device)\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].float().unsqueeze(1).to(device)\n",
        "\n",
        "                outputs = model(images, input_ids, attention_mask)\n",
        "\n",
        "                # Probs -> Preds\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                preds = (probs > 0.5).float()\n",
        "\n",
        "                y_true.extend(labels.cpu().numpy().flatten())\n",
        "                y_pred.extend(preds.cpu().numpy().flatten())\n",
        "\n",
        "    # 2. Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # 3. Classification Report (Precision, Recall, F1 per class)\n",
        "    # Note: 0 = NonPolitical, 1 = Political based on your map\n",
        "    target_names = ['Non-Political', 'Political']\n",
        "    report = classification_report(y_true, y_pred, target_names=target_names, digits=4)\n",
        "\n",
        "    # 4. Overall Metrics\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    # --- OUTPUTS ---\n",
        "    print(\"\\n--- Confusion Matrix ---\")\n",
        "    print(cm)\n",
        "    print(\"\\n[Row: True, Col: Predicted]\")\n",
        "    print(f\"TN: {cm[0][0]} | FP: {cm[0][1]}\")\n",
        "    print(f\"FN: {cm[1][0]} | TP: {cm[1][1]}\")\n",
        "\n",
        "    print(\"\\n--- Class-wise Metrics ---\")\n",
        "    print(report)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Final Accuracy: {acc*100:.2f}%\")\n",
        "    print(f\"Final Macro F1: {macro_f1:.4f}\")\n",
        "    print(\"==========================================\")\n",
        "\n",
        "    # Optional: Plot Confusion Matrix if in Notebook\n",
        "    try:\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=target_names, yticklabels=target_names)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(\"Skipping plot (could not render).\")\n",
        "\n",
        "def main():\n",
        "    # ==========================================\n",
        "    # 1. CONFIGURATION\n",
        "    # ==========================================\n",
        "    MERGED_CSV_PATH = \"/content/drive/MyDrive/CUETPoliMemeDecode/PIXAI/PIXAI_extracted_caption.csv\"\n",
        "    IMAGES_DIR = \"/content/drive/MyDrive/CUETPoliMemeDecode/PIXAI/Image\"\n",
        "    SAVE_PATH = \"/content/temp\"\n",
        "\n",
        "    if not os.path.exists(SAVE_PATH):\n",
        "        os.makedirs(SAVE_PATH)\n",
        "\n",
        "    TASK_NAME = \"task1\"\n",
        "    MAX_LEN = 64\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 5\n",
        "    LR_RATE = 2e-5\n",
        "    NUM_HEADS = 8\n",
        "\n",
        "    print(\"==========================================\")\n",
        "    print(f\"Initializing Pipeline: Political vs Non-Political\")\n",
        "    print(\"==========================================\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. DATA LOADING (70-15-15 Split)\n",
        "    # ==========================================\n",
        "    train_loader, val_loader, test_loader, class_weights = load_dataset_merged(\n",
        "        MERGED_CSV_PATH,\n",
        "        IMAGES_DIR,\n",
        "        MAX_LEN,\n",
        "        BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. TRAINING\n",
        "    # ==========================================\n",
        "    # Train and save the best model\n",
        "    train(\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        TASK_NAME,\n",
        "        SAVE_PATH,\n",
        "        NUM_HEADS,\n",
        "        class_weights,\n",
        "        EPOCHS,\n",
        "        LR_RATE\n",
        "    )\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. FINAL EVALUATION ON TEST SET\n",
        "    # ==========================================\n",
        "    print(\"\\nLoading Best Saved Model for Testing...\")\n",
        "\n",
        "    # 1. Initialize fresh model structure\n",
        "    best_model = PIXAI(clip_model, num_classes=1, num_heads=NUM_HEADS, max_len=MAX_LEN)\n",
        "    best_model = best_model.to(device)\n",
        "\n",
        "    # 2. Load the best weights saved during training\n",
        "    best_model_path = os.path.join(SAVE_PATH, f'model_{TASK_NAME}.pth')\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # 3. Run detailed metrics on the Test Loader (15%)\n",
        "    get_detailed_metrics(best_model, test_loader, device)\n",
        "\n",
        "    print(\"\\nPipeline Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T19:40:32.130636Z",
          "iopub.execute_input": "2025-12-03T19:40:32.131218Z",
          "iopub.status.idle": "2025-12-03T20:06:34.193421Z",
          "shell.execute_reply.started": "2025-12-03T19:40:32.131182Z",
          "shell.execute_reply": "2025-12-03T20:06:34.192321Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YnBYXpJRaXtc",
        "outputId": "758599a1-31ec-42b1-bff5-3e4e77d574cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================\n",
            "Initializing Pipeline: Political vs Non-Political\n",
            "==========================================\n",
            "Loading Single Merged Dataset from /content/drive/MyDrive/CUETPoliMemeDecode/PIXAI/PIXAI_extracted_caption.csv...\n",
            "Splitting: 70% Train / 15% Val / 15% Test\n",
            "Total Samples: 2860\n",
            "Training Samples: 2002\n",
            "Validation Samples: 429\n",
            "Test Samples: 429\n",
            "Detected Max Sequence Length: 64\n",
            "Using Weighted Loss (Pos Weight: 1.6767)\n",
            "Start Training DORA (Binary Mode)\n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5 [Train]: 100%|██████████| 126/126 [02:30<00:00,  1.20s/batch, loss=0.604]\n",
            "Epoch 1/5 [Valid]: 100%|██████████| 27/27 [01:14<00:00,  2.75s/batch, loss=0.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Results Epoch 1 ---\n",
            "Train | Loss: 0.6038 | Acc: 79.87% | Macro F1: 0.7397\n",
            "Valid | Loss: 0.3797 | Acc: 90.21% | Macro F1: 0.8841\n",
            "------------------------------\n",
            ">> Model Saved (Best Macro F1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5 [Train]: 100%|██████████| 126/126 [02:28<00:00,  1.18s/batch, loss=0.312]\n",
            "Epoch 2/5 [Valid]: 100%|██████████| 27/27 [00:16<00:00,  1.62batch/s, loss=0.261]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Results Epoch 2 ---\n",
            "Train | Loss: 0.3124 | Acc: 91.41% | Macro F1: 0.8969\n",
            "Valid | Loss: 0.2605 | Acc: 94.64% | Macro F1: 0.9352\n",
            "------------------------------\n",
            ">> Model Saved (Best Macro F1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5 [Train]: 100%|██████████| 126/126 [02:19<00:00,  1.11s/batch, loss=0.195]\n",
            "Epoch 3/5 [Valid]: 100%|██████████| 27/27 [00:14<00:00,  1.82batch/s, loss=0.213]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Results Epoch 3 ---\n",
            "Train | Loss: 0.1954 | Acc: 94.96% | Macro F1: 0.9399\n",
            "Valid | Loss: 0.2125 | Acc: 94.64% | Macro F1: 0.9346\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5 [Train]: 100%|██████████| 126/126 [02:19<00:00,  1.11s/batch, loss=0.131]\n",
            "Epoch 4/5 [Valid]: 100%|██████████| 27/27 [00:14<00:00,  1.81batch/s, loss=0.222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Results Epoch 4 ---\n",
            "Train | Loss: 0.1311 | Acc: 95.95% | Macro F1: 0.9520\n",
            "Valid | Loss: 0.2220 | Acc: 95.10% | Macro F1: 0.9403\n",
            "------------------------------\n",
            ">> Model Saved (Best Macro F1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5 [Train]: 100%|██████████| 126/126 [02:19<00:00,  1.11s/batch, loss=0.0939]\n",
            "Epoch 5/5 [Valid]: 100%|██████████| 27/27 [00:14<00:00,  1.83batch/s, loss=0.235]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Results Epoch 5 ---\n",
            "Train | Loss: 0.0939 | Acc: 97.40% | Macro F1: 0.9691\n",
            "Valid | Loss: 0.2351 | Acc: 94.64% | Macro F1: 0.9340\n",
            "------------------------------\n",
            "Best Validation Macro F1: 0.9403\n",
            "\n",
            "Loading Best Saved Model for Testing...\n",
            "\n",
            "==========================================\n",
            "Running Comprehensive Test Evaluation\n",
            "==========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 27/27 [01:04<00:00,  2.39s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Confusion Matrix ---\n",
            "[[295   6]\n",
            " [ 23 105]]\n",
            "\n",
            "[Row: True, Col: Predicted]\n",
            "TN: 295 | FP: 6\n",
            "FN: 23 | TP: 105\n",
            "\n",
            "--- Class-wise Metrics ---\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Non-Political     0.9277    0.9801    0.9532       301\n",
            "    Political     0.9459    0.8203    0.8787       128\n",
            "\n",
            "     accuracy                         0.9324       429\n",
            "    macro avg     0.9368    0.9002    0.9159       429\n",
            " weighted avg     0.9331    0.9324    0.9309       429\n",
            "\n",
            "------------------------------\n",
            "Final Accuracy: 93.24%\n",
            "Final Macro F1: 0.9159\n",
            "==========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pipeline Complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"HEY\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-03T20:06:53.454218Z",
          "iopub.execute_input": "2025-12-03T20:06:53.454500Z",
          "iopub.status.idle": "2025-12-03T20:06:53.459752Z",
          "shell.execute_reply.started": "2025-12-03T20:06:53.454478Z",
          "shell.execute_reply": "2025-12-03T20:06:53.459014Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wIEJFwtaXtc",
        "outputId": "67e6f557-11f0-4a2c-bb3b-5b5572f4c5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HEY\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "e5s0KDXouOwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/temp/model_task1.pth /content/drive/MyDrive/CUETPoliMemeDecode/PIXAI/models/epoch5"
      ],
      "metadata": {
        "id": "eWGvFQ9r1eW2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}